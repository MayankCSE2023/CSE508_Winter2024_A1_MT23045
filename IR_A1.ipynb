{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkYjwCiUQqnp",
        "outputId": "fabd98a4-be95-4d77-95d5-bb4fc8f87583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "3vvnVBMRTUZH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path ='/content/drive/MyDrive/IR dataset/text_files'\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "UzSofW8fWxbY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all text files in the folder\n",
        "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]\n"
      ],
      "metadata": {
        "id": "NbDiM0lqW8tB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the list of file paths\n",
        "#file_paths"
      ],
      "metadata": {
        "id": "ojHxCPqnYJjd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n"
      ],
      "metadata": {
        "id": "ene0pD6EYMSe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq8u7V8fZGzT",
        "outputId": "1072fb37-2743-4815-a7c2-9aa53feaf798"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new directory for preprocessed files\n",
        "preprocessed_folder = '/content/drive/MyDrive/IR dataset/preprocessed_files'\n",
        "os.makedirs(preprocessed_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "Keg2JMApaNi-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove ellipses (\"...\") and words with apostrophes attached (\"'m\", \"'ve\", etc.)\n",
        "    text = re.sub(r'\\.\\.\\.', '', text)\n",
        "    text = re.sub(r'\\s*\\'[a-z]*\\s*', ' ', text)\n",
        "\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    tokens = [token for token in tokens if token.strip() != '']\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n"
      ],
      "metadata": {
        "id": "cf4s9wGcZLOX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocessing files\n",
        "def preprocess_all_files(file_paths, preprocessing_function):\n",
        "\n",
        "    processed_count = 0\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content_before = file.read()\n",
        "\n",
        "            if processed_count < 5:\n",
        "              print(f\"File Content Before Preprocessing ({file_path}):\")\n",
        "              print(content_before)\n",
        "\n",
        "            # Perform preprocessing\n",
        "            preprocessed_content = preprocessing_function(content_before)\n",
        "\n",
        "            # Save the preprocessed content to a new file in the preprocessed folder\n",
        "            output_file_name = os.path.basename(file_path).replace('.txt', '_preprocessed.txt')\n",
        "            output_path = os.path.join(preprocessed_folder, output_file_name)\n",
        "\n",
        "            with open(output_path, 'w') as output_file:\n",
        "                output_file.write(preprocessed_content)\n",
        "\n",
        "            if processed_count < 5:\n",
        "              print(f\"\\nFile Content After Preprocessing ({output_path}):\")\n",
        "              print(preprocessed_content)\n",
        "              print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "            processed_count += 1\n",
        "\n",
        "# Call the function to preprocessing\n",
        "preprocess_all_files(file_paths, preprocess_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeD09PXK0Pe6",
        "outputId": "c707a78c-0039-4238-89d6-a9a6c549e445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Content Before Preprocessing (/content/drive/MyDrive/IR dataset/text_files/file890.txt):\n",
            "My 3rd Joyo Pedal, I'm falling in love with that company, solid great sounding pedals for a fraction of other brands.\n",
            "Be advised, the effect of this pedal is very subtle... I use it with my mustang V, which already models other amps, this pedal just makes it sound way more realistic and adds some dynamics to your playing.\n",
            " Totally worth it for anybody that wants to improve their tone... You have to have a good ear though.\n",
            "I've read some reviews of people hooking this pedal before their amp input... since this pedal has it's own pre amp, this way of hooking it up will produce some noise.\n",
            " Connect it to your FX send return.\n",
            "I also own the American sound .... both great... depending on my mood I play them both equally... You just can't go wrong with Joyo\n",
            "\n",
            "File Content After Preprocessing (/content/drive/MyDrive/IR dataset/preprocessed_files/file890_preprocessed.txt):\n",
            "3rd joyo pedal falling love company solid great sounding pedals fraction brands advised effect pedal subtle use mustang v already models amps pedal makes sound way realistic adds dynamics playing totally worth anybody wants improve tone good ear though read reviews people hooking pedal amp input since pedal pre amp way hooking produce noise connect fx send return also american sound great depending mood play equally go wrong joyo\n",
            "\n",
            "==================================================\n",
            "\n",
            "File Content Before Preprocessing (/content/drive/MyDrive/IR dataset/text_files/file225.txt):\n",
            "The cover is very beautiful, mainly the interior. One con I find is that it is too heavy and uncomfortable to transport.\n",
            "The main reason why I rate it with 1 stars is that the first week of use (very careful) one of the hinges broke. I can not find another explanation than poor product quality\n",
            "\n",
            "File Content After Preprocessing (/content/drive/MyDrive/IR dataset/preprocessed_files/file225_preprocessed.txt):\n",
            "cover beautiful mainly interior one con find heavy uncomfortable transport main reason rate 1 stars first week use careful one hinges broke find another explanation poor product quality\n",
            "\n",
            "==================================================\n",
            "\n",
            "File Content Before Preprocessing (/content/drive/MyDrive/IR dataset/text_files/file903.txt):\n",
            "This guitar is perfect in every way! It's so easy to play and sounds fantastic! Perfectly balanced with zero neck dive. The bomb inlays just look cool! I definitely plan on buying more Hardluck Kings guitars!\n",
            "\n",
            "File Content After Preprocessing (/content/drive/MyDrive/IR dataset/preprocessed_files/file903_preprocessed.txt):\n",
            "guitar perfect every way easy play sounds fantastic perfectly balanced zero neck dive bomb inlays look cool definitely plan buying hardluck kings guitars\n",
            "\n",
            "==================================================\n",
            "\n",
            "File Content Before Preprocessing (/content/drive/MyDrive/IR dataset/text_files/file902.txt):\n",
            "Beautiful, well-made strap. Matches my guitar nicely. Perfect.\n",
            "\n",
            "File Content After Preprocessing (/content/drive/MyDrive/IR dataset/preprocessed_files/file902_preprocessed.txt):\n",
            "beautiful well-made strap matches guitar nicely perfect\n",
            "\n",
            "==================================================\n",
            "\n",
            "File Content Before Preprocessing (/content/drive/MyDrive/IR dataset/text_files/file199.txt):\n",
            "Amazing and highly different Wah from ibanez,This is the WAH YA WANT!!,too many reasons!,Just check it out!!!\n",
            "\n",
            "File Content After Preprocessing (/content/drive/MyDrive/IR dataset/preprocessed_files/file199_preprocessed.txt):\n",
            "amazing highly different wah ibanez wah ya want many reasons check\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "kFMH53A3Zq0B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths\n",
        "original_files_path = '/content/drive/MyDrive/IR dataset/text_files'\n",
        "preprocessed_files_path = '/content/drive/MyDrive/IR dataset/preprocessed_files'"
      ],
      "metadata": {
        "id": "aU4HcarxikQs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inverted_index(folder_path):\n",
        "    inverted_index = {}\n",
        "    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "            tokens = word_tokenize(content)\n",
        "\n",
        "            for token in set(tokens):  # Use set to remove duplicate tokens in a document\n",
        "                file_name = os.path.basename(file_path).replace('_preprocessed.txt', '.txt')\n",
        "\n",
        "                if token not in inverted_index:\n",
        "                    inverted_index[token] = {file_name}\n",
        "                else:\n",
        "                    inverted_index[token].add(file_name)\n",
        "\n",
        "    return inverted_index"
      ],
      "metadata": {
        "id": "DC_9W8HsTbLT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save inverted index using pickle\n",
        "def save_inverted_index(inverted_index, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(inverted_index, file)\n"
      ],
      "metadata": {
        "id": "M78EUCAQQ8KQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load inverted index using pickle\n",
        "def load_inverted_index(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        inverted_index = pickle.load(file)\n",
        "    return inverted_index"
      ],
      "metadata": {
        "id": "LRuzDXLMRBb0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unigram inverted index and save it\n",
        "inverted_index = create_inverted_index(preprocessed_files_path)\n",
        "save_inverted_index(inverted_index, '/content/drive/MyDrive/IR dataset/inverted_index.pkl')\n"
      ],
      "metadata": {
        "id": "XesR_eVfShjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "8c45755c-51cd-4785-e2d6-87e21f7d6e34"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ad8a591f82b6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create unigram inverted index and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minverted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_inverted_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_files_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msave_inverted_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/IR dataset/inverted_index.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-f4be307520c4>\u001b[0m in \u001b[0;36mcreate_inverted_index\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the inverted index\n",
        "loaded_inverted_index = load_inverted_index('/content/drive/MyDrive/IR dataset/inverted_index.pkl')"
      ],
      "metadata": {
        "id": "DiGQghN3ne3R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print the first few entries of the loaded inverted index for verification\n",
        "for term, postings in list(loaded_inverted_index.items())[:5]:\n",
        "    print(f\"{term}: {postings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvzI63JjTdlU",
        "outputId": "87cac477-3bac-4647-93ce-fd3fd6933d34"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mat: {'file523.txt', 'file2.txt'}\n",
            "around: {'file796.txt', 'file244.txt', 'file559.txt', 'file268.txt', 'file174.txt', 'file73.txt', 'file14.txt', 'file825.txt', 'file118.txt', 'file187.txt', 'file578.txt', 'file163.txt', 'file934.txt', 'file686.txt', 'file159.txt', 'file901.txt', 'file957.txt', 'file712.txt', 'file738.txt', 'file790.txt', 'file994.txt', 'file802.txt', 'file196.txt', 'file325.txt', 'file326.txt', 'file986.txt', 'file821.txt', 'file404.txt', 'file5.txt', 'file47.txt', 'file849.txt', 'file31.txt', 'file839.txt', 'file558.txt', 'file911.txt', 'file915.txt', 'file953.txt', 'file886.txt', 'file109.txt', 'file880.txt', 'file541.txt', 'file43.txt', 'file70.txt', 'file170.txt', 'file366.txt', 'file124.txt', 'file100.txt', 'file833.txt', 'file256.txt', 'file2.txt', 'file363.txt', 'file6.txt', 'file248.txt', 'file524.txt', 'file830.txt', 'file294.txt', 'file343.txt', 'file951.txt', 'file679.txt', 'file35.txt', 'file993.txt'}\n",
            "enough: {'file815.txt', 'file930.txt', 'file302.txt', 'file11.txt', 'file21.txt', 'file47.txt', 'file980.txt', 'file258.txt', 'file567.txt', 'file96.txt', 'file216.txt', 'file588.txt', 'file827.txt', 'file813.txt', 'file487.txt', 'file232.txt', 'file993.txt', 'file838.txt', 'file649.txt', 'file937.txt', 'file861.txt', 'file781.txt', 'file18.txt', 'file129.txt', 'file738.txt', 'file684.txt', 'file379.txt', 'file610.txt', 'file141.txt', 'file305.txt', 'file643.txt', 'file413.txt', 'file434.txt', 'file256.txt', 'file749.txt', 'file4.txt', 'file465.txt', 'file597.txt', 'file988.txt', 'file144.txt', 'file936.txt', 'file616.txt', 'file20.txt', 'file944.txt', 'file163.txt', 'file642.txt', 'file912.txt', 'file213.txt', 'file886.txt', 'file573.txt', 'file260.txt', 'file665.txt', 'file766.txt', 'file834.txt', 'file853.txt', 'file116.txt', 'file208.txt', 'file634.txt', 'file521.txt', 'file3.txt', 'file883.txt', 'file172.txt', 'file324.txt', 'file885.txt', 'file222.txt', 'file342.txt', 'file340.txt', 'file863.txt', 'file855.txt', 'file462.txt', 'file303.txt', 'file264.txt', 'file253.txt', 'file2.txt', 'file668.txt', 'file979.txt', 'file146.txt', 'file627.txt', 'file77.txt'}\n",
            "organization: {'file2.txt'}\n",
            "works: {'file50.txt', 'file161.txt', 'file878.txt', 'file90.txt', 'file227.txt', 'file261.txt', 'file686.txt', 'file784.txt', 'file563.txt', 'file939.txt', 'file458.txt', 'file393.txt', 'file184.txt', 'file293.txt', 'file567.txt', 'file133.txt', 'file555.txt', 'file682.txt', 'file428.txt', 'file938.txt', 'file322.txt', 'file330.txt', 'file487.txt', 'file724.txt', 'file173.txt', 'file773.txt', 'file453.txt', 'file406.txt', 'file356.txt', 'file32.txt', 'file332.txt', 'file448.txt', 'file135.txt', 'file647.txt', 'file267.txt', 'file684.txt', 'file801.txt', 'file254.txt', 'file780.txt', 'file79.txt', 'file466.txt', 'file368.txt', 'file347.txt', 'file857.txt', 'file897.txt', 'file147.txt', 'file708.txt', 'file644.txt', 'file620.txt', 'file354.txt', 'file414.txt', 'file580.txt', 'file408.txt', 'file597.txt', 'file631.txt', 'file941.txt', 'file35.txt', 'file874.txt', 'file703.txt', 'file472.txt', 'file30.txt', 'file945.txt', 'file823.txt', 'file693.txt', 'file867.txt', 'file333.txt', 'file712.txt', 'file353.txt', 'file994.txt', 'file810.txt', 'file377.txt', 'file886.txt', 'file512.txt', 'file776.txt', 'file317.txt', 'file34.txt', 'file600.txt', 'file595.txt', 'file594.txt', 'file450.txt', 'file130.txt', 'file968.txt', 'file645.txt', 'file285.txt', 'file45.txt', 'file467.txt', 'file323.txt', 'file390.txt', 'file709.txt', 'file276.txt', 'file44.txt', 'file192.txt', 'file811.txt', 'file568.txt', 'file2.txt', 'file605.txt', 'file344.txt', 'file632.txt', 'file417.txt', 'file760.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_query(query):\n",
        "    return preprocess_text(query)"
      ],
      "metadata": {
        "id": "LaUWea6coaC1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_boolean_query(query_terms, logical_operators):\n",
        "    boolean_query = ''\n",
        "\n",
        "    for i, term in enumerate(query_terms):\n",
        "        if i > 0:\n",
        "            boolean_query += f' {logical_operators[i-1]} '\n",
        "\n",
        "        boolean_query += term\n",
        "\n",
        "    return boolean_query"
      ],
      "metadata": {
        "id": "Mc0iVH1fxHF5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform AND operation between two sets of document IDs\n",
        "def perform_and_operation(set1, set2):\n",
        "    return set1.intersection(set2)"
      ],
      "metadata": {
        "id": "xN5Fsm1PhrFn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform OR operation between two sets of document IDs\n",
        "def perform_or_operation(set1, set2):\n",
        "    return set1.union(set2)\n"
      ],
      "metadata": {
        "id": "Ak5SuJNWhs_u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform AND NOT operation between two sets of document IDs\n",
        "def perform_and_not_operation(set1, set2):\n",
        "    return set1.difference(set2)"
      ],
      "metadata": {
        "id": "Iaju6vIRh460"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform OR NOT operation between two sets of document IDs\n",
        "def perform_or_not_operation(set1, set2, all_documents):\n",
        "    return all_documents.difference(set2).union(set1)"
      ],
      "metadata": {
        "id": "B4NEy-lVh-zj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_query(query, inverted_index, all_documents):\n",
        "    # Split the query into terms and operators\n",
        "    query_parts = re.findall(r'\\b\\w+\\b|[^\\w\\s]', query)\n",
        "# Split the query into terms and operators\n",
        "    query_parts = re.findall(r'\\b\\w+\\b|[^\\w\\s]', query)\n",
        "\n",
        "    # Combine consecutive operator terms into a single entity\n",
        "    combined_query_parts = []\n",
        "    current_operator = None\n",
        "    for part in query_parts:\n",
        "        if part.isupper():\n",
        "            # Capitalized term, likely an operator\n",
        "            if current_operator is None:\n",
        "                current_operator = part\n",
        "            else:\n",
        "                current_operator += ' ' + part\n",
        "        else:\n",
        "            # Lowercase term, add current_operator if present\n",
        "            if current_operator is not None:\n",
        "                combined_query_parts.append(current_operator)\n",
        "                current_operator = None\n",
        "            combined_query_parts.append(part)\n",
        "\n",
        "    # If an operator is present at the end, add it\n",
        "    if current_operator is not None:\n",
        "        combined_query_parts.append(current_operator)\n",
        "\n",
        "    #print(combined_query_parts)\n",
        "\n",
        "    # Initialize the result set with all documents\n",
        "    result_set = set(all_documents)\n",
        "\n",
        "    operator = None\n",
        "    temp_result_set = set()  # Temporary set for intermediate results\n",
        "\n",
        "    for part in combined_query_parts:#['coffee', 'AND', 'brewing', 'OR NOT', 'techniques', 'OR', 'cookbook']\n",
        "        if part in ['AND', 'OR', 'NOT','AND NOT', 'OR NOT']:\n",
        "            operator = part\n",
        "        else:\n",
        "            # Retrieve the inverted index for the next term\n",
        "            term = part\n",
        "            term_set = set(inverted_index.get(term, {}))\n",
        "\n",
        "\n",
        "            #print(term_set)\n",
        "\n",
        "            # Perform operations based on the operator\n",
        "            if operator == 'AND':\n",
        "                temp_result_set = perform_and_operation(result_set, term_set)\n",
        "            elif operator == 'OR':\n",
        "                temp_result_set = perform_or_operation(result_set, term_set)\n",
        "            elif operator == 'NOT':\n",
        "                temp_result_set = perform_and_not_operation(result_set, term_set)\n",
        "            elif operator == 'AND NOT':\n",
        "                temp_result_set = perform_and_not_operation(result_set, term_set)\n",
        "            elif operator == 'OR NOT':\n",
        "                temp_result_set = perform_or_not_operation(result_set, term_set,set(all_documents))\n",
        "            else:\n",
        "                 temp_result_set=term_set\n",
        "\n",
        "            # Update the main result set\n",
        "            result_set = temp_result_set.copy()\n",
        "            #print(result_set)\n",
        "\n",
        "    # Store the results for this query\n",
        "    result = {\n",
        "        'query_text': query,\n",
        "        'num_documents_retrieved': len(result_set),\n",
        "        'documents_retrieved': list(result_set)\n",
        "    }\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "td6HNE5yl8V5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample queries\n",
        "# queries = [\n",
        "#     'They are as good quality wise as much more expensive',#good quality wise much expensive\n",
        "#     'AND,AND,AND,AND',\n",
        "#     'Car bag in a canister',\n",
        "#     'OR, AND NOT'\n",
        "# ]"
      ],
      "metadata": {
        "id": "pMftupWAr5KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take user input\n",
        "N = int(input(\"Enter the number of queries (N): \"))\n",
        "queries = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8316iVHc_1hs",
        "outputId": "9972bf8a-f3aa-4cb1-9433-b39efe8da99f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of queries (N): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(N):\n",
        "    query_sequence = input(\"Enter the input sequence: \")\n",
        "    operations = input(\"Enter operations separated by comma: \")\n",
        "    queries.extend([query_sequence, operations])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlDjsYxo_6W-",
        "outputId": "3800d68b-247b-46e3-ca7c-135d2c1a90e4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the input sequence: They are as good quality wise as much more expensive\n",
            "Enter operations separated by comma: AND,AND,AND,AND\n",
            "Enter the input sequence: Car bag in a canister\n",
            "Enter operations separated by comma: OR, AND NOT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and execute queries\n",
        "for i in range(0, len(queries), 2):\n",
        "    preprocessed_query = preprocess_query(queries[i])\n",
        "\n",
        "    # Split the preprocessed query into terms\n",
        "    query_terms = preprocessed_query.split()\n",
        "\n",
        "    # Split the logical operators\n",
        "    logical_operators = queries[i + 1].split(',')\n",
        "\n",
        "    # Convert to boolean query\n",
        "    boolean_query = convert_to_boolean_query(query_terms, logical_operators)\n",
        "\n",
        "    #print(f\"\\nConverted boolean query for query {i // 2 + 1}: {boolean_query}\")\n",
        "\n",
        "     # Get the list of all documents from the inverted index\n",
        "    all_documents = set()\n",
        "    for term, document_set in loaded_inverted_index.items():\n",
        "        all_documents.update(document_set)\n",
        "\n",
        "    # Execute query\n",
        "    query_result = execute_query(boolean_query, loaded_inverted_index, all_documents)\n",
        "\n",
        "    # Print the result\n",
        "    print(f\"Query: {query_result['query_text']}\")\n",
        "    print(f\"Number of documents retrieved: {query_result['num_documents_retrieved']}\")\n",
        "    print(f\"Names of the documents retrieved: {', '.join(query_result['documents_retrieved'])}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amExwRCTAdiF",
        "outputId": "8ba77bb5-5fce-4542-bf01-016281040101"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: good AND quality AND wise AND much AND expensive\n",
            "Number of documents retrieved: 1\n",
            "Names of the documents retrieved: file8.txt\n",
            "\n",
            "Query: car OR bag  AND NOT canister\n",
            "Number of documents retrieved: 31\n",
            "Names of the documents retrieved: file956.txt, file886.txt, file3.txt, file573.txt, file174.txt, file459.txt, file73.txt, file698.txt, file930.txt, file542.txt, file118.txt, file699.txt, file682.txt, file746.txt, file864.txt, file466.txt, file665.txt, file313.txt, file686.txt, file892.txt, file264.txt, file942.txt, file738.txt, file166.txt, file797.txt, file981.txt, file863.txt, file404.txt, file363.txt, file860.txt, file780.txt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create unigram inverted index\n",
        "def create_positional_index(folder_path):\n",
        "    positional_index = {}\n",
        "    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "            tokens = word_tokenize(content)\n",
        "\n",
        "            for position, token in enumerate(tokens):\n",
        "                file_name = os.path.basename(file_path).replace('_preprocessed.txt', '.txt')\n",
        "\n",
        "                if token not in positional_index:\n",
        "                    positional_index[token] = {file_name: [position]}\n",
        "                else:\n",
        "                    if file_name not in positional_index[token]:\n",
        "                        positional_index[token][file_name] = [position]\n",
        "                    else:\n",
        "                        positional_index[token][file_name].append(position)\n",
        "\n",
        "    return positional_index\n"
      ],
      "metadata": {
        "id": "97E4ctvsy5sN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save inverted index using pickle\n",
        "def save_positional_index(positional_index, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(positional_index, file)\n"
      ],
      "metadata": {
        "id": "eP4ifqffQnah"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load inverted index using pickle\n",
        "def load_positional_index(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        positional_index = pickle.load(file)\n",
        "    return positional_index"
      ],
      "metadata": {
        "id": "67YuLXdGV0_t"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unigram inverted index and save it\n",
        "positional_index = create_positional_index(preprocessed_files_path)\n",
        "save_positional_index(positional_index, '/content/drive/MyDrive/IR dataset/positional_index.pkl')"
      ],
      "metadata": {
        "id": "SlMjR0QAV3S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the inverted index\n",
        "loaded_positional_index = load_positional_index('/content/drive/MyDrive/IR dataset/positional_index.pkl')"
      ],
      "metadata": {
        "id": "Y8tmScQ9nz55"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print the first few entries of the loaded inverted index for verification\n",
        "for term, postings in list(loaded_positional_index.items())[:5]:\n",
        "    print(f\"{term}: {postings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOiR7nBAV2k4",
        "outputId": "da091f09-90fa-4a58-d68d-db2fb2991733"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3rd: {'file890.txt': [0], 'file444.txt': [34], 'file507.txt': [66], 'file130.txt': [33]}\n",
            "joyo: {'file890.txt': [1, 68], 'file369.txt': [6], 'file342.txt': [65], 'file947.txt': [18], 'file513.txt': [52], 'file836.txt': [6, 28], 'file222.txt': [37]}\n",
            "pedal: {'file890.txt': [2, 14, 22, 43, 47], 'file470.txt': [19, 26], 'file410.txt': [21], 'file390.txt': [55, 66, 72, 77, 91, 100], 'file453.txt': [27, 41], 'file47.txt': [6], 'file369.txt': [1], 'file305.txt': [52, 56, 70], 'file665.txt': [75, 102], 'file342.txt': [4, 24, 33, 47, 57], 'file668.txt': [0, 16], 'file521.txt': [41], 'file865.txt': [1, 25], 'file333.txt': [4, 6, 8], 'file325.txt': [3, 57, 71], 'file19.txt': [39, 48], 'file429.txt': [16], 'file947.txt': [13, 34], 'file684.txt': [0, 17], 'file610.txt': [23], 'file79.txt': [8, 22, 27, 38, 67], 'file883.txt': [36, 46], 'file258.txt': [9, 20], 'file629.txt': [3, 24, 40], 'file423.txt': [2], 'file941.txt': [27], 'file614.txt': [17, 21], 'file12.txt': [22], 'file968.txt': [2, 29], 'file461.txt': [45], 'file683.txt': [2], 'file846.txt': [17, 19, 21, 24], 'file860.txt': [3], 'file493.txt': [2], 'file324.txt': [68], 'file816.txt': [2], 'file602.txt': [5], 'file513.txt': [5, 21, 30, 44], 'file194.txt': [7, 22], 'file703.txt': [2, 58, 254], 'file130.txt': [1, 50], 'file852.txt': [2], 'file190.txt': [4, 8], 'file191.txt': [42], 'file823.txt': [12], 'file964.txt': [3], 'file911.txt': [48, 52], 'file220.txt': [1, 9, 12, 21, 35], 'file828.txt': [0, 42, 55], 'file979.txt': [12], 'file850.txt': [12, 52], 'file252.txt': [0, 3, 23, 30, 40], 'file997.txt': [19], 'file545.txt': [0], 'file441.txt': [8, 13, 21, 28, 34], 'file575.txt': [1, 60], 'file727.txt': [2], 'file988.txt': [29], 'file354.txt': [6], 'file836.txt': [1, 5, 15], 'file981.txt': [34, 49], 'file775.txt': [3], 'file951.txt': [27, 32, 58], 'file336.txt': [17, 51], 'file367.txt': [10, 31], 'file760.txt': [59], 'file55.txt': [1], 'file571.txt': [28], 'file756.txt': [41], 'file533.txt': [4, 9], 'file847.txt': [37, 58], 'file945.txt': [11], 'file223.txt': [5, 16]}\n",
            "falling: {'file890.txt': [3], 'file325.txt': [23], 'file301.txt': [36], 'file150.txt': [43], 'file269.txt': [67], 'file45.txt': [67], 'file483.txt': [21], 'file749.txt': [37], 'file789.txt': [80], 'file267.txt': [33]}\n",
            "love: {'file890.txt': [4], 'file122.txt': [11], 'file812.txt': [2], 'file331.txt': [0, 1, 2], 'file908.txt': [33], 'file305.txt': [30], 'file630.txt': [0], 'file880.txt': [0], 'file744.txt': [48], 'file333.txt': [0], 'file64.txt': [7, 59], 'file675.txt': [1], 'file729.txt': [0], 'file148.txt': [4], 'file947.txt': [12, 33], 'file770.txt': [79], 'file171.txt': [4], 'file29.txt': [69], 'file519.txt': [0], 'file121.txt': [0, 10], 'file212.txt': [12], 'file819.txt': [0], 'file111.txt': [28], 'file457.txt': [17], 'file934.txt': [1], 'file467.txt': [98], 'file146.txt': [0], 'file551.txt': [6], 'file755.txt': [5], 'file894.txt': [8], 'file961.txt': [55], 'file549.txt': [0], 'file506.txt': [0], 'file57.txt': [0], 'file400.txt': [83], 'file478.txt': [0], 'file140.txt': [4], 'file214.txt': [67], 'file340.txt': [0], 'file289.txt': [1], 'file968.txt': [7], 'file269.txt': [69], 'file473.txt': [0], 'file525.txt': [0], 'file37.txt': [66], 'file973.txt': [35], 'file247.txt': [0, 15], 'file624.txt': [0], 'file31.txt': [30], 'file349.txt': [0], 'file415.txt': [0], 'file273.txt': [23], 'file324.txt': [84, 86], 'file635.txt': [0], 'file178.txt': [0], 'file46.txt': [17], 'file621.txt': [60], 'file194.txt': [18, 28], 'file637.txt': [6], 'file48.txt': [44], 'file517.txt': [52, 53], 'file520.txt': [20], 'file483.txt': [42], 'file590.txt': [0], 'file279.txt': [2, 20], 'file681.txt': [58], 'file541.txt': [72], 'file536.txt': [23], 'file689.txt': [10], 'file445.txt': [0, 7], 'file403.txt': [4], 'file384.txt': [5, 12], 'file586.txt': [0], 'file77.txt': [18], 'file397.txt': [1], 'file435.txt': [2], 'file474.txt': [0], 'file798.txt': [18], 'file156.txt': [69], 'file851.txt': [3], 'file314.txt': [75], 'file396.txt': [35], 'file243.txt': [17], 'file411.txt': [11], 'file114.txt': [38], 'file280.txt': [5], 'file23.txt': [1], 'file663.txt': [103], 'file413.txt': [64, 125], 'file530.txt': [4], 'file209.txt': [0], 'file55.txt': [4], 'file593.txt': [14], 'file240.txt': [28], 'file638.txt': [12], 'file533.txt': [0], 'file587.txt': [2], 'file119.txt': [18], 'file847.txt': [5], 'file471.txt': [0], 'file304.txt': [7], 'file924.txt': [20], 'file807.txt': [0], 'file554.txt': [2], 'file982.txt': [0], 'file223.txt': [0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_positional_query(query, positional_index):\n",
        "    # Split the query into terms\n",
        "    query_terms = re.findall(r'\\b\\w+\\b', query)\n",
        "\n",
        "    # Initialize the result set with all documents containing the first term\n",
        "    first_term = query_terms[0]\n",
        "    result_set = set(positional_index.get(first_term, {}).keys())\n",
        "\n",
        "    # Iterate through the rest of the terms\n",
        "    for term in query_terms[1:]:\n",
        "        term_positions = positional_index.get(term, {})\n",
        "        #print(term_positions)\n",
        "\n",
        "        # Ensure the documents in the result set have the correct positional relationship\n",
        "        result_set_copy = result_set.copy()  # Create a copy before iterating\n",
        "        for doc in result_set_copy:\n",
        "            if doc not in term_positions:\n",
        "                result_set.discard(doc)\n",
        "            else:\n",
        "                positions_current = term_positions[doc]\n",
        "                positions_previous = positional_index.get(query_terms[query_terms.index(term) - 1], {}).get(doc, [])\n",
        "\n",
        "                # Check if there is a position in current term that is greater than positions of the previous term\n",
        "                if not any(pos_current - pos_previous == 1 for pos_current in positions_current for pos_previous in positions_previous):\n",
        "                    result_set.discard(doc)\n",
        "\n",
        "    # Store the results for this query\n",
        "    result = {\n",
        "        'query_text': query,\n",
        "        'num_documents_retrieved': len(result_set),\n",
        "        'documents_retrieved': list(result_set) if result_set is not None else []\n",
        "    }\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "Qx9Wy9lJ5Cbl"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take input from user\n",
        "num_queries = int(input(\"Enter the number of queries: \"))\n",
        "queries = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxwpVTzK7RNp",
        "outputId": "0f4405c7-baee-4e9c-b5cd-78930d5d6fb0"
      },
      "execution_count": 98,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of queries: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(num_queries):\n",
        "    query = input(\"Enter a phrase query: \")\n",
        "    queries.append(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1plmm2JX7Syh",
        "outputId": "a025a6d3-c1dc-404e-ed70-e59e4eeb4427"
      },
      "execution_count": 99,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a phrase query: its more on toy side than on instrument side, and made in Indonesia.\n",
            "Enter a phrase query: They are as good quality wise as much more expensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the queries\n",
        "preprocessed_queries = [preprocess_query(q) for q in queries]\n",
        "print(preprocessed_queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gQrb7ee7cD9",
        "outputId": "043bf86d-cd9f-47a8-e1a2-c1c2c25a876c"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['toy side instrument side made indonesia', 'good quality wise much expensive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Sample queries\n",
        "# queries = [\n",
        "#     'its more on toy side than on instrument side, and made in Indonesia.',\n",
        "#'They are as good quality wise as much more expensive'\n",
        "#     #'great price good quality'\n",
        "#]"
      ],
      "metadata": {
        "id": "7aEuhQ--sOgQ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute queries\n",
        "for i, query in enumerate(preprocessed_queries):\n",
        "    query_result = execute_positional_query(query, loaded_positional_index)\n",
        "    print(f\"Number of documents retrieved for query {i + 1} using positional index: {query_result['num_documents_retrieved']}\")\n",
        "    print(f\"Names of documents retrieved for query {i + 1} using positional index: {', '.join(query_result['documents_retrieved'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LmJKB4ysZ74",
        "outputId": "8e48f1d2-40bc-4326-bf21-334a575cf079"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents retrieved for query 1 using positional index: 1\n",
            "Names of documents retrieved for query 1 using positional index: file6.txt\n",
            "Number of documents retrieved for query 2 using positional index: 1\n",
            "Names of documents retrieved for query 2 using positional index: file8.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VxTvx7lDss1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}