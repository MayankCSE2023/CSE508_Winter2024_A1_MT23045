# -*- coding: utf-8 -*-
"""IR_A1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1csCXkREmdvbgtg-08rAOU7i_bftScf1Z
"""

from google.colab import drive
drive.mount('/content/drive')

import os

folder_path ='/content/drive/MyDrive/IR dataset/text_files'
os.chdir(folder_path)

# List all text files in the folder
file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]

# Display the list of file paths
#file_paths

import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Create a new directory for preprocessed files
preprocessed_folder = '/content/drive/MyDrive/IR dataset/preprocessed_files'
os.makedirs(preprocessed_folder, exist_ok=True)

# Preprocessing function
def preprocess_text(text):
    # Lowercase the text
    text = text.lower()

    # Remove ellipses ("...") and words with apostrophes attached ("'m", "'ve", etc.)
    text = re.sub(r'\.\.\.', '', text)
    text = re.sub(r'\s*\'[a-z]*\s*', ' ', text)


    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords and punctuation
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]

    # Remove blank space tokens
    tokens = [token for token in tokens if token.strip() != '']

    # Join the tokens back into a string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Function to preprocessing files
def preprocess_all_files(file_paths, preprocessing_function):

    processed_count = 0

    for file_path in file_paths:
        with open(file_path, 'r') as file:
            content_before = file.read()

            if processed_count < 5:
              print(f"File Content Before Preprocessing ({file_path}):")
              print(content_before)

            # Perform preprocessing
            preprocessed_content = preprocessing_function(content_before)

            # Save the preprocessed content to a new file in the preprocessed folder
            output_file_name = os.path.basename(file_path).replace('.txt', '_preprocessed.txt')
            output_path = os.path.join(preprocessed_folder, output_file_name)

            with open(output_path, 'w') as output_file:
                output_file.write(preprocessed_content)

            if processed_count < 5:
              print(f"\nFile Content After Preprocessing ({output_path}):")
              print(preprocessed_content)
              print("\n" + "="*50 + "\n")

            processed_count += 1

# Call the function to preprocessing
preprocess_all_files(file_paths, preprocess_text)

import pickle

# Set paths
original_files_path = '/content/drive/MyDrive/IR dataset/text_files'
preprocessed_files_path = '/content/drive/MyDrive/IR dataset/preprocessed_files'

def create_inverted_index(folder_path):
    inverted_index = {}
    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]

    for file_path in file_paths:
        with open(file_path, 'r') as file:
            content = file.read()
            tokens = word_tokenize(content)

            for token in set(tokens):  # Use set to remove duplicate tokens in a document
                file_name = os.path.basename(file_path).replace('_preprocessed.txt', '.txt')

                if token not in inverted_index:
                    inverted_index[token] = {file_name}
                else:
                    inverted_index[token].add(file_name)

    return inverted_index

# Function to save inverted index using pickle
def save_inverted_index(inverted_index, filename):
    with open(filename, 'wb') as file:
        pickle.dump(inverted_index, file)

# Function to load inverted index using pickle
def load_inverted_index(filename):
    with open(filename, 'rb') as file:
        inverted_index = pickle.load(file)
    return inverted_index

# Create unigram inverted index and save it
inverted_index = create_inverted_index(preprocessed_files_path)
save_inverted_index(inverted_index, '/content/drive/MyDrive/IR dataset/inverted_index.pkl')

# Load the inverted index
loaded_inverted_index = load_inverted_index('/content/drive/MyDrive/IR dataset/inverted_index.pkl')

# Print the first few entries of the loaded inverted index for verification
for term, postings in list(loaded_inverted_index.items())[:5]:
    print(f"{term}: {postings}")

def preprocess_query(query):
    return preprocess_text(query)

def convert_to_boolean_query(query_terms, logical_operators):
    boolean_query = ''

    for i, term in enumerate(query_terms):
        if i > 0:
            boolean_query += f' {logical_operators[i-1]} '

        boolean_query += term

    return boolean_query

# Function to perform AND operation between two sets of document IDs
def perform_and_operation(set1, set2):
    return set1.intersection(set2)

# Function to perform OR operation between two sets of document IDs
def perform_or_operation(set1, set2):
    return set1.union(set2)

# Function to perform AND NOT operation between two sets of document IDs
def perform_and_not_operation(set1, set2):
    return set1.difference(set2)

# Function to perform OR NOT operation between two sets of document IDs
def perform_or_not_operation(set1, set2, all_documents):
    return all_documents.difference(set2).union(set1)

def execute_query(query, inverted_index, all_documents):
    # Split the query into terms and operators
    query_parts = re.findall(r'\b\w+\b|[^\w\s]', query)
# Split the query into terms and operators
    query_parts = re.findall(r'\b\w+\b|[^\w\s]', query)

    # Combine consecutive operator terms into a single entity
    combined_query_parts = []
    current_operator = None
    for part in query_parts:
        if part.isupper():
            # Capitalized term, likely an operator
            if current_operator is None:
                current_operator = part
            else:
                current_operator += ' ' + part
        else:
            # Lowercase term, add current_operator if present
            if current_operator is not None:
                combined_query_parts.append(current_operator)
                current_operator = None
            combined_query_parts.append(part)

    # If an operator is present at the end, add it
    if current_operator is not None:
        combined_query_parts.append(current_operator)

    #print(combined_query_parts)

    # Initialize the result set with all documents
    result_set = set(all_documents)

    operator = None
    temp_result_set = set()  # Temporary set for intermediate results

    for part in combined_query_parts:#['coffee', 'AND', 'brewing', 'OR NOT', 'techniques', 'OR', 'cookbook']
        if part in ['AND', 'OR', 'NOT','AND NOT', 'OR NOT']:
            operator = part
        else:
            # Retrieve the inverted index for the next term
            term = part
            term_set = set(inverted_index.get(term, {}))


            #print(term_set)

            # Perform operations based on the operator
            if operator == 'AND':
                temp_result_set = perform_and_operation(result_set, term_set)
            elif operator == 'OR':
                temp_result_set = perform_or_operation(result_set, term_set)
            elif operator == 'NOT':
                temp_result_set = perform_and_not_operation(result_set, term_set)
            elif operator == 'AND NOT':
                temp_result_set = perform_and_not_operation(result_set, term_set)
            elif operator == 'OR NOT':
                temp_result_set = perform_or_not_operation(result_set, term_set,set(all_documents))
            else:
                 temp_result_set=term_set

            # Update the main result set
            result_set = temp_result_set.copy()
            #print(result_set)

    # Store the results for this query
    result = {
        'query_text': query,
        'num_documents_retrieved': len(result_set),
        'documents_retrieved': list(result_set)
    }

    return result

# Sample queries
# queries = [
#     'They are as good quality wise as much more expensive',#good quality wise much expensive
#     'AND,AND,AND,AND',
#     'Car bag in a canister',
#     'OR, AND NOT'
# ]

# Take user input
N = int(input("Enter the number of queries (N): "))
queries = []

for _ in range(N):
    query_sequence = input("Enter the input sequence: ")
    operations = input("Enter operations separated by comma: ")
    queries.extend([query_sequence, operations])

# Preprocess and execute queries
for i in range(0, len(queries), 2):
    preprocessed_query = preprocess_query(queries[i])

    # Split the preprocessed query into terms
    query_terms = preprocessed_query.split()

    # Split the logical operators
    logical_operators = queries[i + 1].split(',')

    # Convert to boolean query
    boolean_query = convert_to_boolean_query(query_terms, logical_operators)

    #print(f"\nConverted boolean query for query {i // 2 + 1}: {boolean_query}")

     # Get the list of all documents from the inverted index
    all_documents = set()
    for term, document_set in loaded_inverted_index.items():
        all_documents.update(document_set)

    # Execute query
    query_result = execute_query(boolean_query, loaded_inverted_index, all_documents)

    # Print the result
    print(f"Query: {query_result['query_text']}")
    print(f"Number of documents retrieved: {query_result['num_documents_retrieved']}")
    print(f"Names of the documents retrieved: {', '.join(query_result['documents_retrieved'])}\n")

# Function to create unigram inverted index
def create_positional_index(folder_path):
    positional_index = {}
    file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]

    for file_path in file_paths:
        with open(file_path, 'r') as file:
            content = file.read()
            tokens = word_tokenize(content)

            for position, token in enumerate(tokens):
                file_name = os.path.basename(file_path).replace('_preprocessed.txt', '.txt')

                if token not in positional_index:
                    positional_index[token] = {file_name: [position]}
                else:
                    if file_name not in positional_index[token]:
                        positional_index[token][file_name] = [position]
                    else:
                        positional_index[token][file_name].append(position)

    return positional_index

# Function to save inverted index using pickle
def save_positional_index(positional_index, filename):
    with open(filename, 'wb') as file:
        pickle.dump(positional_index, file)

# Function to load inverted index using pickle
def load_positional_index(filename):
    with open(filename, 'rb') as file:
        positional_index = pickle.load(file)
    return positional_index

# Create unigram inverted index and save it
positional_index = create_positional_index(preprocessed_files_path)
save_positional_index(positional_index, '/content/drive/MyDrive/IR dataset/positional_index.pkl')

# Load the inverted index
loaded_positional_index = load_positional_index('/content/drive/MyDrive/IR dataset/positional_index.pkl')

# Print the first few entries of the loaded inverted index for verification
for term, postings in list(loaded_positional_index.items())[:5]:
    print(f"{term}: {postings}")

def execute_positional_query(query, positional_index):
    # Split the query into terms
    query_terms = re.findall(r'\b\w+\b', query)

    # Initialize the result set with all documents containing the first term
    first_term = query_terms[0]
    result_set = set(positional_index.get(first_term, {}).keys())

    # Iterate through the rest of the terms
    for term in query_terms[1:]:
        term_positions = positional_index.get(term, {})
        #print(term_positions)

        # Ensure the documents in the result set have the correct positional relationship
        result_set_copy = result_set.copy()  # Create a copy before iterating
        for doc in result_set_copy:
            if doc not in term_positions:
                result_set.discard(doc)
            else:
                positions_current = term_positions[doc]
                positions_previous = positional_index.get(query_terms[query_terms.index(term) - 1], {}).get(doc, [])

                # Check if there is a position in current term that is greater than positions of the previous term
                if not any(pos_current - pos_previous == 1 for pos_current in positions_current for pos_previous in positions_previous):
                    result_set.discard(doc)

    # Store the results for this query
    result = {
        'query_text': query,
        'num_documents_retrieved': len(result_set),
        'documents_retrieved': list(result_set) if result_set is not None else []
    }

    return result

# Take input from user
num_queries = int(input("Enter the number of queries: "))
queries = []

for _ in range(num_queries):
    query = input("Enter a phrase query: ")
    queries.append(query)

# Preprocess the queries
preprocessed_queries = [preprocess_query(q) for q in queries]
print(preprocessed_queries)

# # Sample queries
# queries = [
#     'its more on toy side than on instrument side, and made in Indonesia.',
#'They are as good quality wise as much more expensive'
#     #'great price good quality'
#]

# Execute queries
for i, query in enumerate(preprocessed_queries):
    query_result = execute_positional_query(query, loaded_positional_index)
    print(f"Number of documents retrieved for query {i + 1} using positional index: {query_result['num_documents_retrieved']}")
    print(f"Names of documents retrieved for query {i + 1} using positional index: {', '.join(query_result['documents_retrieved'])}")

